---
title: 吴恩达机器学习系列课程
urlname: dmfp4g
date: '2022-03-04 20:51:39'
updated: '2023-11-09 11:07:58'
author: Jason Ma
categories:
  - Reading
---
[bilibili](https://player.bilibili.com/player.html?bvid=BV164411b7dx)

# 1-2什么是机器学习
![](/images/yuqueAssets/458b973fd96fb910e334dee844dc1a86.png)

机器学习（Machine Learning）一个适当的学习问题定义如下：计算机程序从经验E中学习解决一些任务T，并有性能指标P，T的性能P随着E的增长而提高。



机器学习算法：

+ 监督（Supervised）学习
+ 非监督学习
+ 半监督学习
+ ……

Others：强化（Reinforcement）学习，推荐（Recommender）系统



不同的算法适合解决不同类型的问题，本课程会大量教你如何运用这些算法工具解决实际问题



# 1-3监督学习
![](/images/yuqueAssets/11fb9edd403bcea80eb0f58644460556.png)

监督学习指已给出正确的数据集，需要机器学习推测出更多正确的数据



对于该例子更专业的描述：这是一个（统计学）回归（Regression）问题，

预测连续的数值输出（<font style="color:rgb(221, 221, 221);">Predict continuous valued output</font>）



![](/images/yuqueAssets/4c872efcb4a55405ae7d7073d2cdba11.png)

这个例子是分类（Classification）问题，预测一个离散（Discrete）值的输出。

肿瘤只有两种可能，恶性或良心，预测的结果也只有这两种可能。



![](/images/yuqueAssets/ce36476c901940ec445d9903056803d5.png)



# 1-4无监督学习
![](/images/yuqueAssets/dcf17ef5e7a8f2f06a7e4bc924b46de7.png)![](/images/yuqueAssets/da3245ea3b099e0879e93a396a819d10.png)

无监督学习尝试在没有标记的数据集里找到某种结构

这个例子中算法可能将数据分成两个簇，这就是聚类（Clustering）算法

比如谷歌新闻每天从无数新闻中将同一事件的新闻聚合到一起



![](/images/yuqueAssets/4d646415d2f3497bd40dabacb2ee35e9.png)



![](/images/yuqueAssets/ad1d4b066c5a67facb1ea225ea98119f.png)

此算法将两段不同位置录制的两种声音混在一起的音频，分离成两个单独的音频



# 2-1模型描述
![](/images/yuqueAssets/8f6d98eaeb8cfa572969e90cb3c5f513.png)

![](/images/yuqueAssets/0ac03c0582d4a24e2b3d0d40e6c84ed6.png)

![](/images/yuqueAssets/4d7556847ecb3149f047812dac1dcf72.png)

训练该例子之前根据问题的特征先设计一个假设（Hypothesis）算法：

$ h_\theta(x)=\theta_0+\theta_1 x=y $

这就是一元线性回归 / 单变量（Univariate）线性回归

## 补充：线性
![](/images/yuqueAssets/e5b266d58f26f6f2d8254f42939ac75f.png)





# 2-2代价函数
![](/images/yuqueAssets/9697b0f5f080c88d4e4d0b9b7e2b4b6e.png)

![](/images/yuqueAssets/44bb60d84cf6d9d4d008cd0766d4302a.png)

![](/images/yuqueAssets/4af1bb4b77e88003e6cf13eec464c502.png)

训练要随着经验E的增长而使性能P增长，所以需要一个代价（Cost）函数告诉我们每一次采样的P，而任务T本质上就是找到最优的模型参数θ0、θ1使函数与样本之间的误差最小，也就是最小平方差：

$ J(\theta_0,\theta_1)=\frac{1}{2m}\sum _{i=1}^{m}( h_{\theta }( x_{i}) -y_{i})^{2} $

m是输入样本数量，x是输入（面积），y是输出（价格）



# 2-3代价函数（一）
![](/images/yuqueAssets/b36e8d130688d989dd84c4a61715ccbf.png)

 ![](/images/yuqueAssets/b2aa5adea390a2ba13919954661af7fd.png)![](/images/yuqueAssets/d4bd656cc85d2d474ce46846dc0c7ef7.png)![](/images/yuqueAssets/474c9b986d42836e08bbb5f6295eb8e6.png)



# 2-4代价函数（二）
![](/images/yuqueAssets/61502121be288ef9d14ff3c13887988a.png)

![](/images/yuqueAssets/d2ba03ad3a4ad15a774ba9fe67c9dcb8.png)

![](/images/yuqueAssets/1b3881d054b944f8f42bc269c6f12673.png)![](/images/yuqueAssets/79ef1603994fa27fbbbe38b2ae0d6c1b.png)

所以我们的目标类似函数拟合，给出数据，选择最优类型的函数，算法自动找出使误差最小的参数。

越接近误差最小值，代价函数的梯度越小，反之越大，所以可以根据梯度调整参数，逼近最小值。



# 2-5梯度下降
**Gradient Descent**

![](/images/yuqueAssets/85dc280cef55895ec0a999e4bd12b5d6.png)

![](/images/yuqueAssets/d58397092b034170976514b4ad0f75f1.png)

![](/images/yuqueAssets/42781489cdd894e5ed20f2eb3c2bf097.png)

Colon Equal：$ := $

例如$ a\coloneq b $表示将b的**值赋**给a，与计算机编程语言中的=同义。

数学公式中的=与编程语言中的==同义，表示**等于**（判断）



![](/images/yuqueAssets/df6913cd2d8a8b76d346b9b9eb574bb7.png)

a是学习率（Learning Rate），越大学习越快，下山的步长越大

![](/images/yuqueAssets/7ab4b7aca8a3aa4ac2a0e65102473845.png)![](/images/yuqueAssets/6488e799cd536d2e916fe17eff36671f.png)

我们通常所说的梯度下降使用的迭代方法是对每一项参数计算完之后暂存，然后一次性同步更新（Simultaneous Update），再进入下一次迭代



# 2-6梯度下降知识点总结
![](/images/yuqueAssets/444b7003663b152b0420861a76fb7fac.png)![](/images/yuqueAssets/fbd9156e3442de697ccca3acaa3e1595.png)

![](/images/yuqueAssets/456995762817588fb1c26ae1f9ccd493.png)

梯度下降在一次次迭代中朝着切线的反方向前进，如果速度a太大则可能无法收敛，太小则收敛太慢。



![](/images/yuqueAssets/4775a2c43426d9eaf9c0ae9dd416de66.png)

如果初始参数已经是局部最小值，由于导数约等于零，所以结果不会发生变化。

![](/images/yuqueAssets/b427daef1dafa40b332de014a83a1b1e.png)

当越接近最小值时，梯度会越来越小，自动减小步长，所以随着迭代次数的增多，结果会稳定在最小值



# 2-7线性回归的梯度下降
![](/images/yuqueAssets/32069753e85e80031229e17bff960f57.png)

![](/images/yuqueAssets/aace6ff60bb4b92b0ce2d70d6cb13e2f.png)![](/images/yuqueAssets/8818b4691914f506bdc848affd4d84ce.png)



![](/images/yuqueAssets/a63cff8fd7b5d009048e32cf081ffacf.png)

![](/images/yuqueAssets/05713b443b55973b77cabf643658df7e.png)

这里将梯度下降的公式展开，分别写出θ0、θ1求导后的公式



![](/images/yuqueAssets/ef8437901a93f000d18288bf7031637c.png)![](/images/yuqueAssets/300b61869fb4b53648d7917e2193a3d8.png)

当代价函数是一个标准的凹函数时，不论初始点在哪总是会收敛到全局最小值，否则会遇到局部最小值的问题。

![](/images/yuqueAssets/2f1b2c98c180d995cde170c6b4d76591.png)

这被称为“Batch”梯度下降，Batch指下降的每一步都要遍历整个数据集（m）

还有其他梯度下降的方法，不用遍历全部数据。

对于线性回归问题，另有正态（正规）方程法（Normal Equations Method），可以直接算出代价函数最小值，但只适合比较小的数据集。



## 补充：求导
[https://zh.wikipedia.org/wiki/%E5%AF%BC%E6%95%B0](https://zh.wikipedia.org/wiki/%E5%AF%BC%E6%95%B0)

[bilibili](https://player.bilibili.com/player.html?bvid=BV1Lf4y1M72V)

![](/images/yuqueAssets/3c3dba8345627b2f2d017f2b43b97e73.png)![](/images/yuqueAssets/5718fc57d87e6a71435a07e0eab3fbc2.png)

[https://zh.wikipedia.org/wiki/%E5%AF%BC%E6%95%B0%E5%88%97%E8%A1%A8](https://zh.wikipedia.org/wiki/%E5%AF%BC%E6%95%B0%E5%88%97%E8%A1%A8)![](/images/yuqueAssets/e27fa5ab0bbd4b62a65c9bd28f9b9560.png)

## 补充：复合函数求偏导函数
[bilibili](https://player.bilibili.com/player.html?bvid=BV1gQ4y1X7rL)



# 3-5矩阵乘法特征
矩阵乘法不满足交换律（Commutative Laws），但是满足结合律（Associative Laws）。

略。

# 3-6逆和转置
只有方阵（Square Matrix，行数==列数）才有逆矩阵（Inverse）。

奇异（Singular）矩阵或退化（Degenerate）矩阵不存在逆矩阵。



# 4-1多元线性回归
![](/images/yuqueAssets/98b59b8778db30d6f5ea18bafb283d46.png)

上标加括号的意义是什么呢？

![](/images/yuqueAssets/2e66212bad0d6daccfdd13ccc18965cd.png)

对于多元线性回归（Multivariate Linear Regression）有大量特征的情况，使用矩阵乘法简化计算，两个矩阵分别填充输入特征（Feature）和系数。



# 4-2多元梯度下降法
![](/images/yuqueAssets/897502bf6cb19e5eea95d349278ec351.png)![](/images/yuqueAssets/0be52696dfc64fbbd385adda485a6914.png)

多元梯度下降：

$ \theta _{j} \coloneq \theta _{j} -a\frac{\partial }{\partial \theta _{j}} J( \theta ) $

根据：

假设函数：$ h_\theta(x)=\theta^T x $

代价函数：$ J(\theta)=\frac{1}{2m}\sum _{i=1}^{m}( h_{\theta }( x^{(i)}) -y^{(i)})^{2} $

计算偏导数得：

$ \theta _{j} \coloneq \theta _{j} -a\frac{1}{m}\sum _{i=1}^{m}\left( h_{\theta }\left( x^{( i)}\right) -y^{( i)}\right) x_{j}^{( i)} $

![](/images/yuqueAssets/1269cc3869735d052091606ae3444a12.png)

其中θj表示第j个系数

$ x_{j}^{( i)} $表示第i行第j列的输入特征

x为一整行的输入特征，是一个向量

θ为每个输入特征对应的系数，也是向量



![](/images/yuqueAssets/829d511725dbf3dea735cd70db3628ea.png)

$ x_{j}^{( i)} $是怎么来的：

$ \frac{\partial }{\partial \theta _{j}} J( \theta ) $

计算$ J( \theta ) $的**关于**$ \theta _{j} $偏导数，θ是系数向量，除了θj之外都要看成常数，常数开导为0，得：

$ \theta_j x_j $

求导得：

$ x_j $

带入$ x^{( i)} $得：

$ x_{j}^{( i)} $







# 4-3多元梯度下降法演练：特征缩放


![](/images/yuqueAssets/578a9bca46e4452b7d9971f402521584.png)![](/images/yuqueAssets/9fb74717b573a55cfd1413b404445e83.png)![](/images/yuqueAssets/b595388b79f9bc22398bfacdefa6a594.png)



代价函数的等高线图像越接近圆形，梯度下降的速度越快，若输入特征的取值范围相差非常大，则会造成等高线图像非常狭长，不利于梯度下降。

可以用**特征缩放（Feature Scaling）**和**均值归一化（Mean Normalization）**来使输入特征保持在近似的取值范围：

$ x=\frac{x-\mu }{s} $

μ是x的均值，s是x的范围（最大值-最小值）



# 4-4多元梯度下降法演练：学习率
![](/images/yuqueAssets/44a050e6b953f610e306c8a65aea4230.png)![](/images/yuqueAssets/b97843c25c0f12c8dc3eaa4387b3561b.png)

如果梯度下降工作正常，那么随着迭代次数的增加，梯度会不断减小，通过画出曲线可以帮助我们判断是否已收敛。也可以通过一些自动化的测试判断是否收敛。

![](/images/yuqueAssets/9f28a21a4caaba905c76f6824f033721.png)

如果无法收敛，通常是因为学习率太大。学习率足够小的情况下，每一次迭代梯度都会下降，但迭代次数也会相应增加。

![](/images/yuqueAssets/b057bfc07463f574c9fe5e880175e416.png)

# 4-5特征和多项式回归
![](/images/yuqueAssets/9e202f27bcb6acb43382bf95ab3a6147.png)

输入特征是有潜在优化空间的

![](/images/yuqueAssets/17fe4d14c49bdbeb07409625ea2b3a8c.png)

可以将多项式回归转化成线性回归，转换一下输入特征的形式即可

![](/images/yuqueAssets/0ffc1fb12053236a2685c3eb99e7c372.png)

凭借对函数图像和数据趋势的了解我们可以选择更合适的函数





# 4-6正规方程 （区别与迭代方法的直接解法）
![](/images/yuqueAssets/b213de5bdc01a38dd44b751f3d158a5e.png)

![](/images/yuqueAssets/a8c05fdc1806ac1fc59c071f9e70096d.png)

对于一个简单的J(θ)，可以对其求导，并使其=0即可求出最小值的位置。

对于参数是向量的J(θ)，对于每一个元素求其偏导函数并=0，最终就可以直接得到最小化代价函数的θ值。

![](/images/yuqueAssets/80209a12b003b5901342abec071703aa.png)

![](/images/yuqueAssets/3ce63eaa8494325af5b394dc8a1f61d9.png)

![](/images/yuqueAssets/d4bd351b264e4780da2069deaad5995d.png)

将输入特征X和输出y写成矩阵形式然后矩阵求导做最小二乘法直接得到最小值

![](/images/yuqueAssets/cb4ca55c82e34f678861f58119b10e90.png)

正规方程的问题在于时间复杂度是矩阵维度的立方，当参数上万甚至百万时，计算将会非常耗时。

而梯度下降的时间复杂度只是随维度线性增长





## 补充：矩阵最小二乘法
> 弹幕：
>
> XTX将不一定为方阵且大小不固定的矩阵转换为大小固定为（n+1）*（n+1）的方阵，转换为Ax=z的方程组形式
>
> 并且这里的XTX=A是满秩的，从而可逆，接下来进行对x也就是theat的求解
>
> 最小二乘法的矩阵形式
>
> 
>
> 这个方程是求最佳近似向量，可以参考矩阵论课本
>
> 
>
> 投影矩阵。将y向系数矩阵的列空间投影，得到近似解
>
> 
>
> 此处运用了Moore-Penrose 广义逆矩阵
>
> 课件当中，(XTX)^(-1)XT为X的左逆（M-P广义逆的一种，相对应的还有右逆）
>
> (XTX)^(-1)X 也即左逆存在的条件是X列满秩，相对应的 右逆存在的条件为X行满秩
>

[bilibili](https://player.bilibili.com/player.html?bvid=BV1UK4y1t7bv)

# 4-7正规方程在矩阵不可逆情况下的解决方法


![](/images/yuqueAssets/80bb3eabdaa06fc55fdfaeb8792c54cf.png)

pinv（Pseudo Inverse）无论XTX是否可逆总能给我们想要的θ值：

[https://www.geeksforgeeks.org/differenece-between-inv-and-pinv-functions-in-matlab/](https://www.geeksforgeeks.org/differenece-between-inv-and-pinv-functions-in-matlab/)



![](/images/yuqueAssets/20eab61e4c49daf416cc1538d8ac8c25.png)



> X^T*X出现不可逆的情况原因可能是：
>
> 1. 方阵中的两个维度之间存在线性变换关系，导致方阵不满秩
>
> 2. n（特征数量）相较于m（样本数量）过大，导致其产生的齐次方程组Ax=0不只有零解
>
> 
>

方阵线性相关导致不满秩、无解、不可逆的代数、几何解释：

[https://www.zhihu.com/question/270393340](https://www.zhihu.com/question/270393340)

# 5-6向量化
![](/images/yuqueAssets/4cf77ed0e133674d9e043e93d6c39c8e.png)

一些线性的计算可以可以用向量和矩阵的形式求解

![](/images/yuqueAssets/b08f32f171271c0f1e88a737c3ce1dc3.png)

![](/images/yuqueAssets/dd9181b69279157f236421c60e480f74.png)

遍历求和这种操作尤其适合向量化

# 6-1分类
![](/images/yuqueAssets/a69cf186b2ca9ef7980aa3ff9e10e8f5.png)

![](/images/yuqueAssets/8183825627670edc98d9795cacada91a.png)

对于一个分类问题，比如判断肿瘤是否为恶性，如果使用之前学习的线性回归加阈值的方法可以跑通，但是如果出现几个距离阈值很远的样本，就会使平方差过大导致过拟合。

![](/images/yuqueAssets/66460ef068fc511fb21e66c9ebd3b757.png)

## Logistic Regression分类算法
# 6-2假设陈述
![](/images/yuqueAssets/f264dffde47dd152feef994d47ef773d.png)

假设函数在之前线性回归的基础上加以修改，结果带入Logistic（与Sigmoid同义）函数

![](/images/yuqueAssets/e9164133ae083d5e4a11236ea175d8a5.png)

假设函数结果的意义是：肿瘤为恶性(y=1)在特定输入(输入的Size)下的概率

# 6-3决策边界（Decision Boundary）
![](/images/yuqueAssets/91e8ac89b0bde93eb65fd28e93970f7d.png)

还是以0.5为阈值将连续的概率划分为离散值

![](/images/yuqueAssets/9286ae7e6b36d5388ce6732385bf6db9.png)

对假设函数等量代换后得到的x1+x2=3这根线就是决策边界

当所有θ确定后，假设函数的决策边界也就确定了

![](/images/yuqueAssets/5166c395cd83ef4bb03817d191b20c8e.png)





## 待补充：线性规划
[https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92](https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92)



# 6-4代价函数
![](/images/yuqueAssets/17f5abfda8e3ced774952d1eca37aa4c.png)

![](/images/yuqueAssets/91a6b1266746f08b20ca61e173296013.png)

之前的代价函数现在成了非凸函数，不利于梯度下降

![](/images/yuqueAssets/14dfb9440cf59999af70eb7c7d531d6f.png)

这里使用新的代价函数，使用-log让预测错误时代价无限大，预测正确时代价为0



## 待补充：凸优化
[机器学习必知必会：凸优化](https://zhuanlan.zhihu.com/p/85408804)



# 6-5简化代价函数与梯度下降
![](/images/yuqueAssets/47203371876e8c3af18eb5d7e657ed07.png)

![](/images/yuqueAssets/d105a6fc2cd7040d953a88034b178f81.png)

![](/images/yuqueAssets/cf9908eee038b67ca25c69218a7d625a.png)

Logistic回归公式

假设函数：

$ h_{\theta }( X) =\frac{1}{1+e^{-\theta ^{T} X}} $

代价函数：

$ J( \theta ) =-\frac{1}{m}\sum _{i=1}^{m}\left[ y^{( i)} logh_{\theta }\left( x^{( i)}\right) +\left( 1-y^{( i)}\right) log\left( 1-h_{\theta }\left( x^{( i)}\right)\right)\right] $

梯度下降法，对$ \frac{\partial }{\partial \theta _{j}} J( \theta ) $开导后：

$ \theta _{j} :=\theta _{j} -a\frac{1}{m}\sum _{i=1}^{m}\left( h_{\theta }\left( x^{( i)}\right) -y^{( i)}\right) x_{j}^{( i)} $



假设函数给出的是连续的概率，代价函数使用了极大似然估计



## 补充：统计学中的极大似然估计
似然估计

[https://zh.wikipedia.org/zh-hans/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1](https://zh.wikipedia.org/zh-hans/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1)

[一文搞懂极大似然估计](https://zhuanlan.zhihu.com/p/26614750)

[bilibili](https://player.bilibili.com/player.html?bvid=BV1Hb4y1m7rE)

## 补充：对数
[https://zh.wikipedia.org/wiki/%E5%AF%B9%E6%95%B0](https://zh.wikipedia.org/wiki/%E5%AF%B9%E6%95%B0)

![](/images/yuqueAssets/e671df47652cd370a547c4dc59413e88.png)

![](/images/yuqueAssets/78ab214eb478f13f4531bd3e4895fdfb.png)

## 补充：Logistic回归代价函数的推导
[Logistic回归代价函数的数学推导及实现_空字符（公众号：月来客栈）的博客-CSDN博客_logistics回归的代价函数](https://blog.csdn.net/The_lastest/article/details/78761577)

## 补充：Logistic Regression
[【机器学习】逻辑回归（非常详细）](https://zhuanlan.zhihu.com/p/74874291)

# 6-6高级优化
![](/images/yuqueAssets/73a4833b70da213638e0b9ff25a0dd98.png)

![](/images/yuqueAssets/cb7bbd35d3156cbbf33bf69adbb1daf0.png)

![](/images/yuqueAssets/387b305b258db64e29b779e503dec67c.png)

通常不需要完全理解高级算法（比如共轭梯度法（Conjugate Gradient））的全部细节，直接调用就可以了，我们需要做的是实现计算$ J( \theta ) $和$ \frac{\partial }{\partial \theta _{j}} J( \theta ) $

# 6-7多元分类：一对多
![](/images/yuqueAssets/57d173b77e1c9d905a6f268ba720a874.png)

![](/images/yuqueAssets/afbe9abb901caa8ee7f34c4a572e61a1.png)

![](/images/yuqueAssets/829da119a01b8b7e4a7f0d19c2ac6549.png)

一对多：针对每个类别分别训练模型，在预测新样本时选择概率最高的模型结果

# 7-1过拟合（Over Fitting）问题
![](/images/yuqueAssets/73b95226cdc6114ce5abc6ff268d5b33.png)

当特征过少，比如用直线拟合房价，就会出现欠拟合（Under Fitting），或称作具有高偏差（High Bias）。

一个优秀的模型应当适当（Just Right）拟合，以增强泛化（Generalize，预测新样本）能力。

但特征过多，就会面临过拟合（Over Fitting），或称作高方差（High Variance），失去预测新样本的能力。 

![](/images/yuqueAssets/9bf264aa09afc8f32e9ab4763c6f4ea1.png)

![](/images/yuqueAssets/5edbefc2c7023a2b4d26ec23960f7d9e.png)

为了避免过拟合，我们可以减少特征的数量，这就涉及到特征提取。

理想的特征应该尽可能少，同时尽可能描述样本之间的差异

另一个选择是**正则化（****<font style="color:rgb(221, 221, 221);">Regularization</font>**<font style="color:rgb(221, 221, 221);">），保留所有特征但是减小θj</font>

## 补充：特征提取（<font style="color:rgb(193, 188, 180);background-color:rgb(26, 28, 29);">Feature Extraction</font>）
[机器学习之数据清洗、特征提取与特征选择](https://zhuanlan.zhihu.com/p/34450286)

[https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%B5%E6%8F%90%E5%8F%96](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%B5%E6%8F%90%E5%8F%96)

[https://www.bilibili.com/video/BV1734y117nR?p=3](https://www.bilibili.com/video/BV1734y117nR?p=3)

![](/images/yuqueAssets/07cafe24137f167c8798a2e8fa9b6a7d.png)

# 7-2正则化（<font style="color:rgb(221, 221, 221);">Regularization）</font>代价函数
![](/images/yuqueAssets/7496de46758cf813bff9936cc0726a33.png)

![](/images/yuqueAssets/c44f3d48b6c110f185bccac52e483b85.png)

![](/images/yuqueAssets/1bb231d3a18c105108bcf16703fd5990.png)

正则化通过增大θ过大的惩罚来缩小θ，从而简化参数，平滑曲线，消除过拟合

$ \lambda  $（Lambda）为正则化系数

![](/images/yuqueAssets/d8fcee4e02797de31b127bee14f3f60c.png)

正则化系数过大时会导致欠拟合

**多重选择**会自动选择最合适的正则化系数

# 7-3线性回归的正则化
![](/images/yuqueAssets/fc866c1f3f7b406091fd176bc666bf14.png)

![](/images/yuqueAssets/a5570529af21ec04660dab0a53a5e359.png)

对于θ1-n，在计算代价时加入惩罚，求导并简化后可以直观地看到，相当于每次迭代把θj缩小一点点，说的正式一点就是θj的平方**范数**变小了

## 正规方程的正则化
![](/images/yuqueAssets/194eccf5297254dac9a4170a14e92df1.png)

![](/images/yuqueAssets/70236c4726f4678db387fd174a536e90.png)

结论是在XTX之后加上$ \lambda\begin{bmatrix}
0 &  &  & \\
 & 1 &  & \\
 &  & \ddots  & \\
 &  &  & 1
\end{bmatrix} $就可以实现正规方程法的正则化

这还顺便解决了一些不可逆的问题，比如XTX不满秩的情况下，加上该矩阵后即满秩

# 7-4Logistic回归的正则化
![](/images/yuqueAssets/9e887233cf5a89f0e6de9d3a41a92fa7.png)

![](/images/yuqueAssets/b8306efe964fa2daa563c324b7f32742.png)

不能说很像，只能说一模一样

![](/images/yuqueAssets/325c5a19a13006b5d9cb4026bec535ff.png)























# 注意事项
+ 第五章Octave教程、第十八章应用实例，这两章可以不用学，有点过时了。
+ 原版的octave作业可以不用做，可以做修改过的python版本作业。
+ 如果和吴恩达老师的《深度学习》公开课一起看，第四、五、六周的内容可以直接学习《深度学习》的相关内容。
+ 这个教程建议在三个月内看完，如果有些地方看不懂，没关系，以后用到的时候再回头看看。
+ 这个课程建议配合课程笔记一起看。本站已经提供了笔记下载

